import pandas as pd
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from tqdm import tqdm
import os
import gc
import time
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs("data", exist_ok=True)

DANMAKU_FILE = "data/all_danmaku.csv"
OUT_FILE = "data/video_sentiment_summary.csv"
PROGRESS_FILE = "data/progress_tracker.pkl"

# ä¿æŒåŸæœ‰æ¨¡å‹
MODEL_NAME = "uer/roberta-base-finetuned-jd-binary-chinese"


def setup_device():
    """è®¾ç½®è®¾å¤‡ï¼Œä¼˜å…ˆä½¿ç”¨GPU"""
    if torch.cuda.is_available():
        device = "cuda"
        logger.info("ä½¿ç”¨GPUè¿›è¡Œæ¨ç†")
    else:
        device = "cpu"
        logger.info("ä½¿ç”¨CPUè¿›è¡Œæ¨ç†ï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
    return device


device = setup_device()

print("æ­£åœ¨åŠ è½½BERTæ¨¡å‹...")
try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).to(device)
    model.eval()
except Exception as e:
    logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    exit(1)


# è¿›åº¦è·Ÿè¸ª
def load_progress():
    if os.path.exists(PROGRESS_FILE):
        return pd.read_pickle(PROGRESS_FILE)
    return {"processed_rows": 0}


def save_progress(progress):
    pd.to_pickle(progress, PROGRESS_FILE)


# ä¼˜åŒ–çš„æƒ…æ„Ÿé¢„æµ‹å‡½æ•°
@torch.no_grad()
def predict_sentiment_batch(texts, batch_size=8):
    """æ‰¹é‡é¢„æµ‹æƒ…æ„Ÿï¼Œè‡ªåŠ¨è°ƒæ•´æ‰¹æ¬¡å¤§å°"""
    texts = [text for text in texts if isinstance(text, str) and text.strip()]
    if not texts:
        return np.array([])

    all_probs = []

    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]

        try:
            inputs = tokenizer(
                batch_texts,
                padding=True,
                truncation=True,
                max_length=64,
                return_tensors="pt"
            ).to(device)

            outputs = model(**inputs)
            probs = torch.softmax(outputs.logits, dim=1)[:, 1]
            all_probs.extend(probs.cpu().numpy())

            # æ¸…ç†å†…å­˜
            del inputs, outputs
            if device == "cuda":
                torch.cuda.empty_cache()

        except RuntimeError as e:
            if "out of memory" in str(e) and batch_size > 1:
                logger.warning(f"GPUå†…å­˜ä¸è¶³ï¼Œå‡å°æ‰¹æ¬¡å¤§å°ä»{batch_size}åˆ°{batch_size // 2}")
                return predict_sentiment_batch(texts, batch_size=batch_size // 2)
            else:
                logger.error(f"æ¨ç†å¤±è´¥: {e}")
                # è¿”å›ä¸­æ€§æƒ…æ„Ÿä½œä¸ºfallback
                all_probs.extend([0.5] * len(batch_texts))

    return np.array(all_probs)


def process_danmaku_sentiment():
    """å¤„ç†å¼¹å¹•æƒ…æ„Ÿåˆ†æ"""
    # æ£€æŸ¥è¿›åº¦
    progress = load_progress()
    logger.info(f"ä»è¿›åº¦ç‚¹ç»§ç»­: å·²å¤„ç† {progress['processed_rows']} è¡Œ")

    # è¯»å–å¼¹å¹•æ•°æ®
    if not os.path.exists(DANMAKU_FILE):
        logger.error(f"å¼¹å¹•æ–‡ä»¶ä¸å­˜åœ¨: {DANMAKU_FILE}")
        return

    logger.info("è¯»å–å¼¹å¹•æ•°æ®...")
    try:
        df = pd.read_csv(DANMAKU_FILE)
        df = df.dropna(subset=["text"])
        logger.info(f"å¼¹å¹•æ•°é‡: {len(df)}")
    except Exception as e:
        logger.error(f"è¯»å–å¼¹å¹•æ–‡ä»¶å¤±è´¥: {e}")
        return

    # å¦‚æœå·²æœ‰è¿›åº¦ï¼Œè·³è¿‡å·²å¤„ç†çš„è¡Œ
    start_idx = progress["processed_rows"]
    if start_idx > 0:
        df = df.iloc[start_idx:]
        logger.info(f"è·³è¿‡å‰ {start_idx} è¡Œï¼Œå‰©ä½™ {len(df)} è¡Œå¾…å¤„ç†")

    # åˆ†æ‰¹å¤„ç†
    chunk_size = 2000  # å‡å°æ‰¹æ¬¡å¤§å°é¿å…å†…å­˜é—®é¢˜
    all_sentiments = []

    for chunk_start in tqdm(range(0, len(df), chunk_size), desc="å¤„ç†å¼¹å¹•"):
        chunk_end = min(chunk_start + chunk_size, len(df))
        chunk = df.iloc[chunk_start:chunk_end]

        # é¢„æµ‹æƒ…æ„Ÿ
        chunk_sentiments = predict_sentiment_batch(chunk["text"].tolist(), batch_size=8)
        all_sentiments.extend(chunk_sentiments)

        # æ›´æ–°è¿›åº¦
        progress["processed_rows"] = start_idx + chunk_end
        save_progress(progress)

        # æ¸…ç†å†…å­˜
        gc.collect()
        if device == "cuda":
            torch.cuda.empty_cache()

        # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡çƒ­
        time.sleep(0.5)

        # æ¯å¤„ç†å‡ ä¸ªå—å°±è¾“å‡ºè¿›åº¦
        if (chunk_start // chunk_size) % 10 == 0:
            logger.info(f"è¿›åº¦: {progress['processed_rows']}/{start_idx + len(df)}")

    # ç¡®ä¿é•¿åº¦åŒ¹é…
    if len(all_sentiments) < len(df):
        all_sentiments.extend([0.5] * (len(df) - len(all_sentiments)))

    df["sentiment"] = all_sentiments[:len(df)]

    # ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
    logger.info("ç”Ÿæˆæƒ…æ„Ÿç‰¹å¾æ±‡æ€»...")
    summary_list = []

    for cid, group in df.groupby("cid"):
        if len(group) < 5:  # å¿½ç•¥å¼¹å¹•å¤ªå°‘çš„è§†é¢‘
            continue

        arr = group.sort_values("time")
        s = arr["sentiment"].values

        summary_list.append({
            "cid": cid,
            "avg_sentiment": float(s.mean()),
            "std_sentiment": float(s.std()),
            "max_sentiment": float(s.max()),
            "min_sentiment": float(s.min()),
            "num_comments": len(s),
            "peak_count": int(sum(
                (s[1:-1] > s[:-2]) & (s[1:-1] > s[2:])
            )),
        })

    # ä¿å­˜ç»“æœ
    summary_df = pd.DataFrame(summary_list)

    # åˆå¹¶å·²æœ‰ç»“æœ
    if os.path.exists(OUT_FILE):
        existing_df = pd.read_csv(OUT_FILE)
        # ç§»é™¤é‡å¤çš„cid
        existing_df = existing_df[~existing_df["cid"].isin(summary_df["cid"])]
        combined_df = pd.concat([existing_df, summary_df], ignore_index=True)
        combined_df.to_csv(OUT_FILE, index=False, encoding="utf-8-sig")
        logger.info(f"åˆå¹¶ç»“æœï¼Œç°æœ‰ {len(combined_df)} ä¸ªè§†é¢‘çš„æƒ…æ„Ÿåˆ†æ")
    else:
        summary_df.to_csv(OUT_FILE, index=False, encoding="utf-8-sig")
        logger.info(f"ä¿å­˜ {len(summary_df)} ä¸ªè§†é¢‘çš„æƒ…æ„Ÿåˆ†æ")

    # æ¸…ç†è¿›åº¦æ–‡ä»¶
    if os.path.exists(PROGRESS_FILE):
        os.remove(PROGRESS_FILE)

    logger.info("æƒ…æ„Ÿåˆ†æå®Œæˆ!")
    return summary_df


# è¿è¡Œä¸»å‡½æ•°
if __name__ == "__main__":
    try:
        result = process_danmaku_sentiment()
        print("\nğŸ‰ æƒ…æ„Ÿåˆ†æå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°ï¼š", OUT_FILE)
    except Exception as e:
        logger.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        print("âŒ å¤„ç†å¤±è´¥ï¼Œä½†è¿›åº¦å·²ä¿å­˜ï¼Œå¯ä»¥é‡æ–°è¿è¡Œç»§ç»­å¤„ç†")